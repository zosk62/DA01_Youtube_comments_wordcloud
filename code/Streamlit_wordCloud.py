#!/usr/bin/env python
# coding: utf-8

# In[5]:


import os
import googleapiclient.discovery
import pandas as pd
from konlpy.tag import Okt

import jpype
from collections import Counter
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt

### í°íŠ¸ ì„¤ì • ë¼ì´ë¸ŒëŸ¬ë¦¬
from matplotlib import font_manager, rc

### í°íŠ¸ ì„¤ì •
plt.rc("font", family="Malgun Gothic")

### ë§ˆì´ë„ˆìŠ¤ê¸°í˜¸ ì„¤ì •
plt.rcParams["axes.unicode_minus"] = False
import streamlit as st
# from streamlit_option_menu import option_menu


# In[7]:


def word_cloud():
    api_service_name = "youtube"
    api_version = "v3"
    DEVELOPER_KEY="AIzaSyCGHrcNmSoUKoRGosiIShFLW9o69acZwi4"
    
    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)
    
    request = youtube.commentThreads().list(
        part="snippet",
        videoId="5E9zOGX3oMo",
        maxResults=500
    )
    
    comments = []
    
    # Execute the request.
    response = request.execute()
    
    # Get the comments from the response.
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']
        public = item['snippet']['isPublic']
        comments.append([
            comment['authorDisplayName'],
            comment['publishedAt'],
            comment['likeCount'],
            comment['textOriginal'],
            public
        ])
    
    while (1 == 1):
      try:
       nextPageToken = response['nextPageToken']
      except KeyError:
       break
      nextPageToken = response['nextPageToken']
      # Create a new request object with the next page token.
      nextRequest = youtube.commentThreads().list(part="snippet", videoId="5E9zOGX3oMo", maxResults=500, pageToken=nextPageToken)
      # Execute the next request.
      response = nextRequest.execute()
      # Get the comments from the next response.
      for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']
        public = item['snippet']['isPublic']
        comments.append([
            comment['authorDisplayName'],
            comment['publishedAt'],
            comment['likeCount'],
            comment['textOriginal'],
            public
        ])
    
    df = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','public'])
    df.info() 
    max_like_count = df['like_count'].max()
    max_like_text = df[df['like_count'] == max_like_count]['text'].values[0]
    print(max_like_text)
    max_like_index = df[df['like_count'] == max_like_count].index[0]
    print(max_like_index)
    save_path="./data/all_comments.csv"
    df.to_csv(save_path, index=False)
    save_path="./data/all_comments.csv"
    df1=pd.read_csv(save_path)    
    okt = Okt()
    all_comment = []
    
    for cmt in df["text"] :
        # print(okt.nouns(cmt))
        ### extend() : ë¦¬ìŠ¤íŠ¸ì— ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ í™•ì¥í•´ì„œ ì¶”ê°€í•˜ëŠ” ë°©ì‹
        #  - append() : ë¦¬ìŠ¤íŠ¸ì— í˜•íƒœ(type) ìì²´ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹
        all_comment.extend(okt.nouns(cmt))
    
    all_comment2 = [w for w in all_comment if len(w) > 1]
    # all_comment2
    all_comment_count = Counter(all_comment2)

    all_top_70 = {}
    for k, v in all_comment_count.most_common(70):
        all_top_70[k] = v
    
    all_top_70 = {k:v for k, v in all_comment_count.most_common(70)}
    # all_top_70
    cmap = plt.get_cmap('viridis')

# Extract values and keys
    values = list(all_top_70.values())
    keys = list(all_top_70.keys())
    
    # Normalize values to use in colormap
    norm = plt.Normalize(min(values), max(values))
    
    # Create a figure and axis
    plt.figure(figsize=(20, 10))
    
    # Title
    plt.title("all ë¦¬ë·°ì˜ ë‹¨ì–´ ìƒìœ„ (50ê°œ) ë¹ˆë„ ì‹œê°í™”", fontsize=17)
    
    # Bar graph with gradient color
    for key, value in zip(keys, values):
        # Additional condition to skip "íƒœì–‘ê´‘" or "ì „ê¸°"
        if key == "íƒœì–‘ê´‘" or key == "ì „ê¸°":
            continue
        
        color = cmap(norm(value))
        plt.bar(key, value, color=color)
    
    # x-axis and y-axis labels
    plt.xlabel("ë¦¬ë·° ëª…ì‚¬")
    plt.ylabel("ë¹ˆë„(count)")
    
    # Adjust x-axis labels rotation
    plt.xticks(rotation=70)
    
    # Show the graph
    plt.show()
    
    st.success('ğŸ“Š Graphical view of the top 50 word frequencies! ğŸ“ˆ')
    st.pyplot(plt)
    # selected_value = st.selectbox('Select Language', ['English','Korean'])
    # if selected_value == 'English':
    #     st.info("""In the context of this analysis, the chosen YouTube video has been selected, 
    #         and all accompanying comments have been systematically extracted and stored in a CSV file. 
    #         Subsequent to this extraction, a comprehensive word analysis has been conducted to discern
    #         patterns and facilitate psychological analysis.""")
    # elif selected_value == 'Korean':
    #     st.text("""
    #             ì´ ë¶„ì„ì˜ ë§¥ë½ì—ì„œëŠ” ì„ íƒëœ YouTube ë¹„ë””ì˜¤ê°€ ì„ ë³„ë˜ì—ˆìœ¼ë©°, ê·¸ì™€ ê´€ë ¨ëœ ëª¨ë“  ëŒ“ê¸€ë“¤ì´ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œë˜ì–´
    #             CSV íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¶”ì¶œ í›„ì—ëŠ” íŒ¨í„´ì„ íŒŒì•…í•˜ê³  ì‹¬ë¦¬ ë¶„ì„ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ í¬ê´„ì ì¸
    #             ë‹¨ì–´ ë¶„ì„ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
    #             """)


    exclude_words = ["íƒœì–‘ê´‘", "ì „ê¸°"]

    
    # Filter out excluded words
    filtered_top_70 = {word: count for word, count in all_top_70.items() if word not in exclude_words}
    
    # WordCloud configuration
    plt.figure(figsize=(8, 8))
    plt.title("ë¦¬ë·° ë‹¨ì–´ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”")
    font_path = "C:/Windows/Fonts/malgunsl.ttf"
    wc = WordCloud(
        font_path=font_path,
        background_color="ivory",
        width=800,
        height=600
    )
    
    # Generate WordCloud from filtered frequencies
    cloud = wc.generate_from_frequencies(filtered_top_70)
    
    # Display WordCloud image
    plt.imshow(cloud)
    plt.axis("off")
    plt.savefig("./img/ë¦¬ë·°_ë‹¨ì–´_ì›Œë“œí´ë¼ìš°ë“œ_ì‹œê°í™”.png")
    plt.show()
    
    st.success('ğŸŒˆ Word cloud visualization of review word frequencies! ğŸ“Š')

    st.pyplot(plt)
    
    with st.expander("Expand to see the Analysis ğŸ“Š", expanded=True):
        selected_value = st.selectbox('Select a language to view the Word Cloud Analysis: ', ['English','Korean'])
        if selected_value == 'English':
            st.text("1. Key words such as 'government', 'generation', 'power', 'production', 'facilities', 'issues', 'policy', 'momentum', 'Moon Jae-in', 'storage', etc., represent topics related to energy generation in Korea.\n"
            "2. Government and Generation Policy: Words like 'government' and 'policy' have high frequencies, and the mention of 'Moon Jae-in' suggests opinions related to the role of the Korean government and policy formulation for energy generation.\n"
            "3. Words like 'generation', 'power', 'production', 'energy', etc., emerged as significant keywords related to energy generation in Korea, indicating the country's focus and importance on energy generation and power production.\n"
            "4. Energy Generation Facilities and Storage Technology: Words like 'facilities', 'power plant', 'hydrogen', 'regeneration', 'nuclear power', 'transmission', 'facilities', 'renewable energy', etc., indicate keywords related to energy generation facilities. The mention of 'storage' reflects an interest in energy storage technology.\n"
            "5. Regional Issues: Words like 'Jeolla-do', 'Honam region', 'Honam', etc., represent issues related to Korea's regional characteristics. There is a possibility that issues related to energy generation in specific regions are mentioned.\n"
            "6. Environment and Issues: The word 'environment' along with words like 'issues', 'inadequate', 'coal', 'pollution', etc., reflects concerns and issues related to environmental problems and inadequate policies in the context of energy generation.\n"
            "7. Other Issues: Words like 'price', 'increase', 'efficiency', 'permit', 'plan', 'deficit', etc., indicate issues related to the economic aspects of energy generation. Terms like price, efficiency, and permits showcase the relevance between energy policy and economic activities.\n"
            "8. Through the above analysis, it is evident that the provided word cloud reflects various topics and issues related to energy generation in Korea. This analysis can help understand opinions and issues related to energy policy formulation and serve as a basis for more effective policies and improvement strategies."
            )
        elif selected_value == 'Korean':
            st.text("""
            1. ì£¼ìš” ë‹¨ì–´ë“¤ì¸ 'ì •ë¶€', 'ë°œì „', 'ì „ë ¥', 'ìƒì‚°', 'ì‹œì„¤', 'ë¬¸ì œ', 'ì •ì±…', 'ê¸°ì„¸', 'ë¬¸ì¬ì¸', 'ì €ì¥' ë“±ì€ í•œêµ­ì˜ ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨ëœ ì£¼ì œë“¤ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤.
            2. ì •ë¶€ì™€ ë°œì „ ì •ì±…: 'ì •ë¶€'ì™€ 'ì •ì±…'ì´ë¼ëŠ” ë‹¨ì–´ë“¤ì´ ë¹ˆë„ìˆ˜ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚˜ë©°, 'ë¬¸ì¬ì¸'ì´ë¼ëŠ” ì¸ë¬¼ë„ ì–¸ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ ì •ë¶€ì˜ ì—­í• ê³¼ ì—ë„ˆì§€ ë°œì „ì— ëŒ€í•œ ì •ì±… ìˆ˜ë¦½ê³¼ ê´€ë ¨ëœ ì˜ê²¬ë“¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
            3. 'ë°œì „', 'ì „ë ¥', 'ìƒì‚°', 'ì—ë„ˆì§€' ë“±ì˜ ë‹¨ì–´ë“¤ì€ í•œêµ­ì˜ ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨ëœ ì£¼ìš” í‚¤ì›Œë“œë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì—ì„œ ì—ë„ˆì§€ ë°œì „ê³¼ ì „ë ¥ ìƒì‚°ì— ëŒ€í•œ ê´€ì‹¬ê³¼ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            4. ì—ë„ˆì§€ ë°œì „ ì‹œì„¤ê³¼ ì €ì¥ ê¸°ìˆ : 'ì‹œì„¤', 'ë°œì „ì†Œ', 'ìˆ˜ì†Œ', 'ì¬ìƒ', 'ì›ì „', 'ì†¡ì „', 'ì„¤ë¹„', 'ì‹ ì¬ìƒì—ë„ˆì§€' ë“±ì˜ ë‹¨ì–´ë“¤ì€ ì—ë„ˆì§€ ë°œì „ ì‹œì„¤ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. 'ì €ì¥'ì´ë¼ëŠ” ë‹¨ì–´ëŠ” ì—ë„ˆì§€ ì €ì¥ ê¸°ìˆ ì— ëŒ€í•œ ê´€ì‹¬ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            5. ì§€ì—­ê³¼ ê´€ë ¨ëœ ì´ìŠˆ: 'ì „ë¼ë„', 'í˜¸ë‚¨ì§€ë°©', 'í˜¸ë‚¨' ë“±ì˜ ë‹¨ì–´ë“¤ì€ í•œêµ­ì˜ ì§€ì—­ì ì¸ íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ì´ìŠˆë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì§€ì—­ë³„ ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨ëœ ë¬¸ì œë“¤ì´ ì–¸ê¸‰ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
            6. í™˜ê²½ê³¼ ë¬¸ì œ: 'í™˜ê²½'ì´ë¼ëŠ” ë‹¨ì–´ì™€ í•¨ê»˜ 'ë¬¸ì œ', 'ë¶€ì‹¤', 'ì„íƒ„', 'ì˜¤ì—¼' ë“±ì˜ ë‹¨ì–´ë“¤ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨í•˜ì—¬ í™˜ê²½ ë¬¸ì œì™€ ë¶€ì‹¤í•œ ì •ì±… ë“±ì— ëŒ€í•œ ìš°ë ¤ì™€ ì´ìŠˆë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.
            7. ê¸°íƒ€ ì´ìŠˆ: 'ê°€ê²©', 'ì¸ìƒ', 'íš¨ìœ¨', 'í—ˆê°€', 'ê³„íš', 'ì ì' ë“±ì˜ ë‹¨ì–´ë“¤ì€ ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨ëœ ê²½ì œì ì¸ ì¸¡ë©´ê³¼ ì´ìŠˆë“¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°€ê²©, íš¨ìœ¨, í—ˆê°€ ë“±ì˜ ë‹¨ì–´ë“¤ì€ ì—ë„ˆì§€ ì •ì±…ê³¼ ê²½ì œ í™œë™ ì‚¬ì´ì˜ ê´€ë ¨ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            8. ìœ„ì˜ ë¶„ì„ì„ í†µí•´ ì£¼ì–´ì§„ ì›Œë“œ í´ë¼ìš°ë“œ ê²°ê³¼ì—ëŠ” í•œêµ­ì˜ ì—ë„ˆì§€ ë°œì „ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì£¼ì œì™€ ì´ìŠˆë“¤ì´ ë°˜ì˜ë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ ì—ë„ˆì§€ ì •ì±… ìˆ˜ë¦½ê³¼ ê´€ë ¨ëœ ì˜ê²¬ ë° ì´ìŠˆë“¤ì„ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ë‹¤ íš¨ê³¼ì ì¸ ì •ì±…ê³¼ ê°œì„  ë°©ì•ˆì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """)



def main():
    
    st.set_page_config(layout="wide")
    st.subheader("ğŸ§ ğŸ’­ ì›Œë“œ í´ë¼ìš°ë“œë¡œ ì‚¬ëŒì˜ ì‹¬ë¦¬ë¥¼ 2ì´ˆë§Œì— ë¶„ì„í•˜ê¸°!")
    st.subheader('ğŸ¯ Objective and Methodology ğŸ“Š')
    with st.expander("View the Analysis", expanded=True):
        
        selected_value = st.selectbox('View in', ['English', 'Korean'])
        if selected_value == 'English':
            st.text("""
            ğŸ“ For this analysis, a specific YouTube video was selected, and its content, along with all associated comments, 
            were systematically extracted and stored in a CSV file. Following the extraction process, a thorough 
            word analysis was conducted to identify patterns and enable psychological analysis.
            """)
        elif selected_value == 'Korean':
            st.text("""
                ğŸ“ ì´ ë¶„ì„ì„ ìœ„í•´ íŠ¹ì • YouTube ë™ì˜ìƒì´ ì„ íƒë˜ì—ˆìœ¼ë©°, í•´ë‹¹ ë™ì˜ìƒì˜ ì½˜í…ì¸ ì™€ ëª¨ë“  ê´€ë ¨ ëŒ“ê¸€ì€ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œë˜ì–´ CSV íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. 
                ì¶”ì¶œ ê³¼ì • ì´í›„ì—ëŠ” ì² ì €í•œ ë‹¨ì–´ ë¶„ì„ì´ ìˆ˜í–‰ë˜ì–´ íŒ¨í„´ì„ ì‹ë³„í•˜ê³  ì‹¬ë¦¬ ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.
                """)
    
    col1, col2= st.columns([1, 1])
    with col1:            
        st.image('./yt.png')
        

    with col2:
        st.subheader("The Comment with the most like is: ")
        st.warning("ì¸ì¦ í•„ìš”í•œ ì‚°ì—…ì— ì „ë ¥ì„ ìµœëŒ€í•œ ë³´ë‚´ì¤˜ì•¼ì§€.....")
   
    word_cloud()

     
               
 



if __name__ == '__main__':
    main()



# In[ ]:




